---
title: "Titanic-in-R"
author: "Everett Robinson"
date: "June 5, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(caret)
```

## Titanic in R

This is my submission for the Kaggle titanic competition: <https://www.kaggle.com/c/titanic>. It leverages the caret package in R, and uses many different models that ultimately have their predictions stacked into a final ensemble model. I have had a lot of success using random forests on the Coursera Practical Machine Learning project <https://www.coursera.org/learn/practical-machine-learning>. My hope is that this combined model will perform better than any one of the models individually.

### Data Import and Exploration
The first step is to import the provided training and test data sets.

```{r import}
train <- read.csv("../input/train.csv")
test <- read_csv("../input/test.csv")

dim(train)
dim(test)
```

Ideally the training data could be split into a new training set and a cross validation set resuting in a standard split of approximately 75% training and 25% cross validation data. This would allow us to examine the out of sample error on a wide variety of models and parameters before commiting to one of them and running it on the test data. Unfortunately at only 891 rows of data, I have a gut feeling that we will need as much training data as we can get away with. I will not split off a cross validation data set from the training data for this project, and instead utilize caret's buildt in capacity for resampling or k-fold crossvalidation. 

Ultimately I will submit the model that performs best on the provided test data set. This should work out in the end, because to the best of my knowledge, Kaggle has reserved another 418 rows of data for final model testing. That can be the real test of whether my model overfitted or not. The only downside is that we'll need to wait three years to find out.

Before we get ahead of ourselves, we should do some data exploration. We'll need to know what data we are actually working with:
```{r column_names}
names(train)
```

We have a numeric PassengerId column that appears to start at 1 and increase by one sequencially to 891 based on the min, max. and median values. This is the sort of data that appears to mostly matter for book keeping when we go to submit the predictions at the end, and that we shouldn't expect to be useful when training. In the worst case it may play a significant role in classification and lead to overfitting. For these reasons we will exclude it from the training steps.

We know from the competition page that he rest of the data is the following:


+---------+----------------------------------------------+----------------+
|Variable |	Definition                                   | Key            |
+=========+=============================================++================+
|survival | Survival                                     | 0 = No,        |
|         |                                              | 1 = Yes        |
+---------+----------------------------------------------+----------------+
|pclass 	| Ticket class                                 | 1 = 1st,       |
|         |                                              | 2 = 2nd,       |
|         |                                              | 3 = 3rd        |
+---------+----------------------------------------------+----------------+
|sex 	    | Sex                                          |                |
+---------+----------------------------------------------+----------------+
|Age 	    | Age in years                                 |                |
+---------+----------------------------------------------+----------------+
|sibsp 	  | num of siblings / spouses aboard the Titanic |                |	
+---------+----------------------------------------------+----------------+
|parch 	  | num of parents / children aboard the Titanic |	              |
+---------+----------------------------------------------+----------------+
|ticket 	| Ticket number                                |                |
+---------+----------------------------------------------+----------------+
|fare 	  | Passenger fare                               |                |
+---------+----------------------------------------------+----------------+
|cabin 	  | Cabin number                                 |                |
+---------+----------------------------------------------+----------------+
|embarked | Port of Embarkation                          | C = Cherbourg, |
|         |                                              | Q = Queenstown,|
|         |                                              | S = Southampton|
+---------+----------------------------------------------+----------------+

**Variable Notes**

**pclass**: A proxy for socio-economic status (SES)
1st = Upper
2nd = Middle
3rd = Lower

**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5

**sibsp**: The dataset defines family relations in this way...
Sibling = brother, sister, stepbrother, stepsister
Spouse = husband, wife (mistresses and fiancÃ©s were ignored)

**parch**: The dataset defines family relations in this way...
Parent = mother, father
Child = daughter, son, stepdaughter, stepson
Some children travelled only with a nanny, therefore parch=0 for them.

The first two variables Survived and Pclass are exactly as described.
```{r}
unique(train$Survived)
unique(train$Pclass)
```
We know from above that Pclass is a categorical variable where the values are first, second, and third class. There are no other possible values. Because Pclass is categorical we can convert it into three separate dummy variable columns later during preprocessing. 


```{r}
summary(train)
```

We can see fromthe summary table that Sex is strictly male and female, and skews towards males. There doesn't appear to be any missing values, and the data is good the way it is.

```{r}
ggplot(data = train) + geom_bar(aes(x = Sex)) + ggtitle("Distribution of Gender on the Voyage")
```

Age is a bit of a different story. There are 177 rows for which age is not available. We will probably need to find a way to impute values for these. Lets see if there is some correlation between age and survival rate:
```{r}
ageData <- train
ageData$AgeBins <- train$Age %>% cut_interval(length = 2.5)
ggplot(data =  ageData %>% group_by(AgeBins) %>% summarise(meanSurv = mean(Survived))) + geom_col(aes(x = AgeBins, y = meanSurv)) + coord_flip()
```

We can see that the one eighty year old survived, making that bar stand out. We also see that young children and early adolescents seem to have done well, and that survival generally decreases with age. Let's break this down by gender:
```{r}
ggplot(data =  ageData %>% filter(Sex == "male") %>% group_by(AgeBins) %>% summarise(meanSurv = mean(Survived))) + geom_col(aes(x = AgeBins, y = meanSurv)) + coord_flip() + ggtitle("Survival Rates for Males")
```

The above pattern is far more striking when we isolate for males. Boys did better, but survival rates tend to crash for those above the age of 15. 

```{r}
ggplot(data =  ageData %>% filter(Sex == "female") %>% group_by(AgeBins) %>% summarise(meanSurv = mean(Survived))) + geom_col(aes(x = AgeBins, y = meanSurv)) + coord_flip() + ggtitle("Survival Rates for Females")
```
For women, the trend is essentially non-existant, and the variability overwhelms any signal that might exist in the noise.

It's good to know that women and children were indeed were indeed prioritized on the titanic and that wasn't just a line from the movie. This also means we shoud expect age and gender to play major roles in survival later in ou trained models.


